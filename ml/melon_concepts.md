# 西瓜书概念
括号表示概念出现的其他页码, 如有兴趣协同整理，请到[issue](https://github.com/ahangchen/windy-afternoon/issues/2)中认领章节
## 第一章 绪论
- Page2: 标记（label)

    示例结果的信息，例如“好瓜”，称为标记
- Page2: 假设(269)(hypothesis)

    学得模型对应了数据的某种潜在的规律，因此亦称假设
- Page2: 示例(instance)

    数据集中的每条记录是关于某个事件或对象的描述，称为一个“示例”或“样本”
- Page2: 属性(attribute)

    反映事务或对象在某方面的表现或性质的事项，如“色泽”，称为属性或特征
- Page2: 属性空间(attribute space)

    属性长成的空间称为属性空间，样本空间，或输入空间
- Page2: 数据集(data set)

    数据记录的集合称为一个数据集
- Page2: 特征(247)(feature)

    同属性
- Page2: 学习(learning)

    从数据中学得模型的过程称为学习或训练
- Page2: 学习器(learner)

    学习过程就是为了找出或逼近真相，有时将模型称作学习器
- Page2: 训练(training)

    同学习
- Page2: 训练集(training data)

    训练过程中使用的数据称为“训练集”，其中每个样本称为一个“训练样本”，训练样本组成的集合称为训练集
- Page2: 训练样本(training sample)

    见训练集
- Page2: 样本(sample)

    同示例
- Page2: 样本空间(sample space)

    同属性空间
- Page2: 样例(sample)

    同示例（instance）
- Page2: 真相(ground-truth)

    潜在规律本身称为真相或真实
- Page3: 标记空间(label space)

    所有标记的集合称为标记空间或输出空间
- Page3: 测试(testing)

    学得模型后，使用其进行预测的过程称为测试，被预测的样本称为测试样本
- Page3: 测试样本(testing sample)

    见测试
- Page3: 簇(197)（cluster）

    将训练集中的西瓜分成若干组，称为聚类，每个组称为一个簇
- Page3: 独立同分布(267)（independent and identically distributed）

    我们获得的每个样本都是独立的从一个分布上采样获得的，即“独立同分布”
- Page3: 多分类（multi-class classification）

    预测值涉及多个类别时，称为“多分类”
- Page3: 二分类（binary classification)

    预测值设计两个分类的任务
- Page3: 泛化（121，350）（generalization)

    学得模型适用于新样本的能力，称为“泛化”能力
- Page3: 分类（classification)

    如果预测的是离散值，此类学习任务称为分类
- Page3: 回归（regression）

    如果预测的值是连续值，此类学习任务称为回归
- Page3: 监督学习（supervised learning）

    根据训练数据是否拥有标记信息，学习任务可以大致分为两大类：监督学习和无监督学习，分类和回归是前者的代表，聚类是后者的代表
- Page3: 聚类(197)（clustering）

    见簇
- Page3: 无导师学习

    同无监督学习
- Page3: 无监督学习(197)（unsupervised learning）

    见有监督学习
- Page3: 有导师学习

    同有监督学习
- Page4: 概念学习(17)（concept learning）

    广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念，因此亦称为概念学习或概念形成
- Page4: 归纳学习(11)（inductive learning）

    从样例中学习
- Page5: 版本空间（version space）

    存在着一个与训练集一致的假设集合，称之为“版本空间”
- Page6: 归纳偏好（inductive bias）

    机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好
- Page6: 偏好

    同归纳偏好
- Page7: 奥卡姆剃刀(17)（Occam's razor）

    若有多个假设与观察一致，则选最简单的那个
- Page10: 符号主义(363)（symbolism）

    基于逻辑表示
- Page10: 连接主义（connectionism）

    基于神经网络
- Page10: 人工智能

    有很多种说法。。见仁见智
- Page11: 机械学习

    信息存储与检索
- Page11: 类比学习

    通过观察和发现学习
- Page11: 示教学习

    从指令中学习
- Page12: 统计学习(139)

    如SVM，核方法
- Page14: 数据挖掘

    从海量数据中发掘知识
- Page16: WEKA
- Page17: 迁移学习

    类比学习升级版

## 第二章 模型评估与选择
- Page23: 错误率(error rate)

    分类错误的样本数占样本总数的比例称为错误率，即如果在m个样本中有a个样本分类错误，则错误率E = a/m；相应的，1-a/m称为精度。
- Page23: 泛化误差（generalization error)

    在新样本上的误差称为泛化误差
- Page23: 过拟合(104,191,352)（overfitting）

    当学习器把训练样本学得太好了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象称为过拟合
- Page23: 过配

    同过拟合
- Page23: 精度(29)(accuracy)

    精度=1-错误率
- Page23: 经验误差(267)(empirical error)

    学习器在训练集上的误差称为“训练误差”
- Page23: 欠配（underfitting）

    欠拟合，对训练样本的一般性质尚未学好
- Page23: 误差(error)

    学习器的实际预测输出与样本的真实输出之间的差异称为误差
- Page23: 训练误差（trainning error)

    同经验误差
- Page24: 模型选择(model selection)

    选择学习算法与参数配置
- Page25: 分层采样(stratified sampling)

    如果从采样的角度看待数据集的划分过程，则保留类别比例的采样方式通常称为“分层采样”
- Page25: 留出法（hold-out）

    直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。
- Page26: k折交叉验证（k-fold cross validation）

    交叉验证先将数据集D划分为k个大小相似的互斥子集，每个自己都尽可能保持数据分布的一致性，即从数据集中分层采样得到，然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，最终返回k个测试结果的均值，交叉验证评估结果的稳定性和保真性很大程度上取决于k的取值，通常称之为k折交叉验证，最常用的k是10
- Page26: 交叉验证法（cross validation）

    同k折交叉验证
- Page27: 包外估计(179)（out of bag estimate）

    用于测试的样本没在训练集中出现，这样的测试结果称为包外估计
- Page27: 自助法(bootstrapping)

    以自主采样法为基础，给定包含m个样本的数据集D，对它采样产生数据集D’：每次随机从D中挑选一个样本，将其考本放入D’， 然后再将该样本放回D中，下次可能再被采到，这个过程执行m次后，得到包含m个样本的数据集D’,m足够大时，有36.8%的样本不会被采到，于是可以用没采到的部分做测试集。
- Page28: 参数调节（parameter tuning）

    大多数学习算法有些参数需要设定，参数配置不同，学得模型的性能往往有显著差别，因此，在进行模型评估与选择时，除了要对适用学习算法进行选择，还需要对算法参数进行设定，这就是参数调节或者调参。
- Page28: 验证集(105)（validation set）

    通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，为了加以区分，模型评估与选择中用于评估测试的数据集常称为“验证集”。
- Page29: 均方误差(54)（mean squared error）

    回归任务最常用的性能度量是均方误差（几何距离）
- Page30: 查全率（recall）

    预测为真且正确的结果占所有预测正确的结果的比例。
- Page30: 查准率（precision）

    预测为真且正确的结果占所有预测结果的比例。
- Page30: 混淆矩阵（confusion matrix）

|真实情况|预测为正例|预测为反例|
| :-:| :-:  | :-:  |
|正例| TP（真正例）| FN（假反例）|
| 反例 | FP（假正例）| TN（真反例）|
- Page30: 召回率

    同查全率
- Page30: 准确率

    同查准率
- Page31: P-R曲线

    查准率（纵轴）与查全率（横轴）的关系曲线
- Page31: 平衡点（break-even point，bep）

    查准率=查全率时的取值。平衡点大的学习模型可以认为综合性能更好
- Page32: F1

    查准率和查全率的调和平均，比算术平均（求和除以2）和几何平均（平方相乘开方）更重视较小值。

    1/F1 = 1/2 (1/P + 1/R)

    1/Fβ = 1/(1+β)（1/P + β²/R）
- Page32: 宏F1(macro-F1)

    如果进行多次训练/测试，每次得到一个混淆矩阵，或是在多个数据集上进行训练/测试，可以在n个混淆矩阵上综合考察查准率和查全率

    macro-P = 1/n(∑Pi)

    macro-R = 1/n(∑Ri)

    1/macro-F1 = 1/2*(1/macro-P + 1/macro-R)

- Page32: 宏查全率

    见宏F1之macro-R
- Page32: 宏查准率


    见宏F1之macro-P

- Page32: 微F1(micro-F1)

    将各混淆矩阵的对应元素进行平均，再去计算，可以得到micro-F1
- Page32: 微查准率

    将各混淆矩阵的对应元素进行平均，再去计算
- Page32: 微查全率

  将各混淆矩阵的对应元素进行平均，再去计算
- Page33: ROC曲线(46)

    真正例率（True Positive Rate，TPR）和假正例率（FPR）的关系曲线

    TPR = TP/(TP+FN)

    FPR = FP/(TN+FP)


- Page35: 代价(47)(cost)

    为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”
- Page35: 代价矩阵

|真实情况|预测为0类|预测为1类|
| :-:| :-:  | :-:  |
|0类| 0| cost01|
|1类| cost10|0|

- Page36: 代价敏感(67)(cost-sensitive)

    在损失函数中考虑了非均等代价
- Page36: 代价曲线

    正例概率代价（横轴）和归一化代价（纵轴）的曲线

    正例概率代价： P(+)cost = p * cost01 /( p * cost01 + (1 - p) * cost10)，p是样例为正例的概率

    归一化代价： cost_norm = (FNR * p * cost01 + FPR * (1-p) * cost10)/(p * cost01+ (1-p) * cost10)
- Page36: 规范化(183)(normalization)

    将不同变化范围的值映射到相同的固定范围中，常见的是[0,1]，此时亦称归一化
- Page36: 归一化（regular）

    同规范化
- Page36: 总体代价

    错误率是直接计算错误次数，并没有考虑不同错误会造成不同的后果，在非均等代价下，我们所希望的不再是简单的最小化错误次数，而是希望最小化总体代价
- Page37: 假设检验(hypothesis test)

    假设是对学习器泛化错误率分布的某种判断或猜想，用测试错误率估计泛化错误率，以检查学习器性能。
- Page38: 二项检验（binomial test）

    二项分布检验，根据收集到的样本数据，推断总体分布是否服从某个指定的二项分布。泛化错误率为e的学习器被测得测试错误率为e’的概率是服从二项分布的。
- Page38: 置信度(confidence)

    估计总体参数落在某一区间时，可能不犯错误的概率，一般用符号1-α表示。
- Page40: 交叉验证成对t校验（paired t-tests）

    对两个学习器A和B，使用k折交叉验证法分别得到k个测试错误率，如果两个学习器性能相同，则使用相同训练/测试集时测试错误率应该相同，求两个学习器的k个测试错误率的差，若abs(sqrt(k)*μ/σ)＜临界值则认为两个学习器性能相同。
- Page41: 5x2交叉验证

    由于交叉验证中，不同轮次的训练集之间有一定程度的重复，会过高估计假设成立的概率，因此做5次2折交叉验证，每次验证前将数据打乱，对5次2对2个学习器的测试错误率求差值，对所有差值求方差，对前两次差值求均值，再进行临界值判断。
- Page41: McNemar检验

    两个学习器分类差别列联表

|算法B\A|正确|错误|
| :-:| :-:  | :-:  |
|正确| e00| e01|
|错误| e10|e11|

检验变量|e01-e10|是否服从正态分布，服从则认为两学习器性能相同等同于检查τx² = (|e01-e10|-1)²/(e01+e10) 是否服从自由度为1的卡方分布（标准正态分布变量的平方）

- Page41: 列联表(187)

    见McNemar检验
- Page42: Friedman检验

    有多个数据集多个学习器进行比较时使用，对各个算法在各个数据集上对测试性能排序，对平均序值计算τx²和τF,并进行临界值检验。
- Page43: Nemenyi后续检验(Nemenyi post-hoc test)

    学习器性能性能显著不同时，进行后续检验来进一步区分各算法，临界值域：CD=qα* sqrt(k*(k+1)/6N)
- Page44: 偏差-方差分解(177)

    对学习算法的期望泛化错误率进行拆解，学习算法在不同训练集上学得的结果很可能不同，真实输出与期望输出的差别称为偏差(bias)，使用样本数相同的不同训练集产生的输出的方差为var(x)，有：E(f;D) = bias²(x) + var(x) + ε²

## 第3章 线性模型
- Page53: 线性回归(252)
- Page53: 线性模型
- Page54: 参数学习
- Page54: 平方损失
- Page54: 最小二乘法(72)
- Page55: 多元线性回归
- Page56: 对数线性回归
- Page56: 正则化(105,133)
- Page57: 对数几率回归
- Page57: 广义线性模型
- Page57: 阶跃函数(98)
- Page57: 联系函数
- Page58: Sigmoid函数(98,102)
- Page58: 对率函数
- Page58: 对率回归(132,325)
- Page58: 对数几率函数(98)
- Page58: 几率
- Page58: 替代函数
- Page59: 对数似然(149)
- Page59: 极大似然法(149,297)
- Page60: Fisher判别分析
- Page60: 线性判别分析(139)
- Page61: 广义瑞利商
- Page61: 类间散度矩阵(138)
- Page61: 类内散度矩阵(138)
- Page62: 全局散度矩阵
- Page63: MvM
- Page63: OVO
- Page63: OvR
- Page63: 多分类器学习
- Page64: ECOC
- Page64: 纠错输出码
- Page65: 编码矩阵
- Page66: 类别不平衡(209)
- Page67: 过采样
- Page67: 欠采样
- Page67: 上采样
- Page67: 稀疏表示(255)
- Page67: 稀疏性
- Page67: 下采样
- Page67: 阈值移动
- Page67: 再平衡
- Page67: 再缩放
- Page68: 多标记学习

## 第四章 决策树
- Page73: 决策树(363)
- Page73: 判定树
- Page74: 分而治之
- Page75: ID3决策树
- Page75: 划分选择
- Page75: 信息增益
- Page77: 增益率
- Page78: C4.5决策树(page83)
- Page79: CART决策树
- Page79: 后剪枝
- Page79: 基尼指数
- Page79: 剪枝(352)
- Page79: 预剪枝(352)
- Page82: 决策树桩
- Page83: 离散化
- Page85: 缺失值
- Page88: 多变量决策树(92)
- Page90: 斜决策树
- Page92: 增量学习(109)

## 第五章 神经网络
- Page97: M-P神经元模型
- Page97: 人工神经网络
- Page97: 神经网络
- Page97: 神经元
- Page97: 阈值(104)
- Page98: 感知机
- Page98: 激活函数
- Page98: 挤压函数
- Page98: 阈值逻辑单元
- Page99: 非线性可分
- Page99: 功能神经元
- Page99: 收敛
- Page99: 线性超平面
- Page99: 线性可分(126)
- Page99: 学习率
- Page99: 哑结点
- Page99: 振荡
- Page100: 多层前馈神经网络
- Page101: BP算法
- Page101: BP网络
- Page101: 单隐层网络
- Page101: 反向传播算法
- Page101: 连接权(104)
- Page101: 误差逆传播
- Page102: 梯度下降（254，389，407
- Page103: 链式法则(402)
- Page105: 累积误差逆传播
- Page105: 早停
- Page106: 参数空间
- Page106: 局部极小
- Page106: 全剧最小
- Page107: 模拟退火
- Page107: 遗传算法
- Page108: ART网络
- Page108: RBF网络
- Page108: 径向基函数
- Page108: 竞争型学习
- Page108: 胜者通吃
- Page108: 自适应谐振理论
- Page109: Kohonen网络
- Page109: SOM网络
- Page109: 可塑性-稳定性窘境
- Page109: 在线学习(241,393)
- Page109: 自组织映射
- Page110: 级联相关
- Page111: Boltzmann分布
- Page111: Boltzmann机
- Page111: Elman网络
- Page111: 递归神经网络
- Page111: 基于能量的模型
- Page112: 对比散度
- Page112: 受限Boltzmann机
- Page113: 发散
- Page113: 卷及神经网络
- Page113: 权共享
- Page113: 深度学习
- Page113: 无监督逐层训练
- Page114: ReLU
- Page114: 表示学习
- Page114: 汇合
- Page114: 特征学习
- Page115: 广义δ规则
- Page115: 可解释性(191)


## 第6章 支持向量机
- Page121: 划分超平面
- Page122: 间隔
- Page122: 支持向量
- Page123: SVM
- Page123: 对偶问题(405)
- Page124: KKT条件(124,132,135)
- Page126: 核函数
- Page127: 核技巧
- Page127: 支持向量展式
- Page128: RKHS
- Page128: 高斯核
- Page128: 核矩阵(138,233)
- Page128: 线性核
- Page128: 再生核希尔伯特空间
- Page129: 软间隔
- Page129: 硬间隔
- Page130: 0/1损失函数(page147)
- Page130: hinge损失
- Page130: 对率损失
- Page130: 松弛变量
- Page130: 替代损失
- Page130: 指数损失(173)
- Page131: 软间隔支持向量机
- Page133: 罚函数法
- Page133: 结构风险
- Page133: 经验风险
- Page133: 支持向量回归
- Page137: Mercer定理
- Page137: 表示定理
- Page137: 核方法
- Page137: 核化(232)
- Page137: 核线性判别分析
- Page139: 割平面法
- Page140: 多核学习
- Page140: 一致性

## 第7章 贝叶斯分类器
- Page147: 贝叶斯风险
- Page147: 贝叶斯最优分类器
- Page147: 风险
- Page147: 条件风险
- Page148: 贝叶斯定理
- Page148: 判别式模型(325)
- Page148: 生成式模型(295,325)
- Page148: 似然
- Page148: 先验
- Page148: 证据
- Page149: 极大似然估计
- Page150: 朴素贝叶斯分类器
- Page150: 条件独立性假设(305)
- Page153: 拉普拉斯修正
- Page154: 半监督贝叶斯分类器
- Page154: 独依赖估计
- Page154: 懒惰学习(225,240)
- Page155: 超父
- Page156: 贝叶斯网(319,339)
- Page156: 概率图模型(319)
- Page156: 信念网
- Page158: V型结构
- Page158: 边际独立性
- Page158: 边际化(328)
- Page158: 道德图
- Page158: 端正图
- Page158: 同父
- Page158: 有向分离
- Page159: 最小描述长度
- Page161: 吉布斯采样(334)
- Page161: 近似推断(161)
- Page161: 精确推断(328,331)
- Page161: 马尔科夫链
- Page161: 平稳分布
- Page162: EM算法(208,295,335)
- Page162: 隐变量(319)
- Page163: 边际似然
- Page163: 坐标下降(408)
- Page164: 贝叶斯分类器
- Page164: 贝叶斯学习

## 第8章 集成学习
- Page171: 多分类器系统
- Page171: 个体学习器
- Page171: 基学习器
- Page171: 基学习算法
- Page171: 集成学习(311)
- Page171: 弱学习器
- Page172: AdaBoost
- Page172: 多样性
- Page172: 投票法(225)
- Page173: Boosting(page139)
- Page173: 加性模型
- Page177: 重采样
- Page177: 重赋权
- Page178: Bagging
- Page178: 自助采样法
- Page179: 随机森林
- Page182: 加权平均(225)
- Page182: 简单平均
- Page182: 绝对多数投票
- Page183: 加权投票(225)
- Page183: 相对多数投票
- Page184: Stacking
- Page185: 贝叶斯模型平均
- Page185: 分歧(304)
- Page185: 误差-分歧分解
- Page187: 差异性度量
- Page187: 多样性度量
- Page189: 属性子集
- Page189: 随机子空间
- Page189: 稳定基学习器
- Page189: 子空间(227)
- Page191: 混合专家
- Page191: 集成修剪
- Page191: 选择性集成
- Page192: Hoeffding不等式(268)

## 第9章 聚类
- Page197: 有效性指标
- Page199: 距离度量
- Page200: 街区距离
- Page200: 离散属性
- Page200: 连续属性
- Page200: 列名属性
- Page200: 曼哈顿距离
- Page200: 闵可夫斯基距离(220)
- Page200: 欧氏距离
- Page200: 切比雪夫距离
- Page200: 数值属性
- Page200: 无序属性
- Page200: 有序属性
- Page201: 非度量距离
- Page201: 混合属性
- Page201: 加权距离
- Page201: 距离度量学习(237)
- Page201: 相似度度量
- Page202: k均值算法(218)
- Page202: 原型聚类
- Page204: LVQ(218)
- Page204: 学习向量化
- Page206: 概率模型(319)
- Page206: 高斯混合(296)
- Page211: 密度聚类
- Page214: 层次聚类
- Page219: 聚类集成
- Page219: 异常检测
- Page220: 豪斯多夫距离

## 第10章 降维与度量学习
- Page225: k近邻
- Page225: 急切学习
- Page225: 平均法
- Page225: 最近邻分类器
- Page226: 密采样
- Page227: 多维缩放
- Page227: 降维
- Page227: 维数约简
- Page227: 维数灾难(247)
- Page229: PCA
- Page229: 线性降维
- Page229: 主成分分析
- Page231: 奇异值分解(402)
- Page232: 本真低维空间
- Page232: 非线性降维
- Page232: 核化线性降维
- Page232: 核主成分分析
- Page234: 本真距离
- Page234: 测地线距离
- Page234: 等度量映射
- Page234: 流形学习
- Page235: 局部线性嵌入
- Page237: 度量学习
- Page238: 近邻成分分析
- Page239: 必连约束(307)
- Page239: 勿连约束
- Page240: 半监督聚类(307)
- Page240: 多视图学习
- Page240: 流形假设(294)
- Page240: 流形正则化

## 第11章 特征选择与稀疏学习
- Page247: 冗余特征
- Page247: 数据预处理
- Page247: 特征选择
- Page247: 相关特征
- Page248: 子集搜索
- Page248: 子集评价
- Page249: 过滤式特征选择
- Page250: 包裹式特征选择
- Page251: 拉斯维加斯方法
- Page251: 蒙特卡洛方法(340,384)
- Page252: LASSO(261)
- Page252: Tikhonov正则化
- Page252: 岭回归
- Page252: 嵌入式特征选择
- Page253: L1正则化
- Page253: L2正则化
- Page253: Lipschitz条件
- Page253: 近端梯度下降(259)
- Page255: 码书学习
- Page255: 稀疏编码
- Page255: 字典学习
- Page257: 压缩感知
- Page259: 局部线性嵌入
- Page259: 协调过滤
- Page260: 核范数
- Page260: 迹范数

## 第12章 计算学习理论
- Page267: 计算学习理论
- Page268: Jensen不等式
- Page268: McDiarmid不等式
- Page268: 概率近似正确
- Page268: 概念类
- Page268: 假设空间
- Page269: PAC辨识
- Page269: PAC可学习
- Page269: 不可分(272)
- Page269: 不一致
- Page269: 可分(270)
- Page269: 时间复杂度
- Page270: PAC学习算法
- Page270: 恰PAC可学习
- Page270: 样本复杂度
- Page270: 有限假设空间
- Page273: VC维(274)
- Page273: 不可知PAC可学习
- Page273: 打散
- Page273: 对分
- Page273: 增长函数
- Page278: 经验风险最小化
- Page279: Rademacher复杂度
- Page284: 稳定性
- Page285: 均匀稳定性

## 第13章 半监督学习
- Page293: 半监督学习(294)
- Page293: 查询
- Page293: 未标记样本
- Page293: 有标记样本
- Page293: 主动学习
- Page294: 聚类假设
- Page295: 直推学习
- Page298: S3VM
- Page298: 半监督SVM
- Page298: 低维嵌入
- Page300: 图半监督学习
- Page301: 亲和矩阵
- Page302: 标记传播
- Page304: 基于分歧的方法
- Page304: 协同训练

## 第14章 概率图模型
- Page319: 马尔科夫网
- Page319: 推断
- Page319: 隐马尔科夫模型
- Page322: 马尔科夫随机场
- Page322: 视图
- Page322: 因子
- Page323: 全局马尔科夫性
- Page324: 局部马尔科夫性
- Page325: 成对马尔科夫性
- Page325: 马尔科夫毯
- Page325: 条件随机场
- Page328: 边际分布
- Page328: 变量消去
- Page330: 信念传播(340)
- Page331: MCMC
- Page333: MH算法
- Page334: 变分推断
- Page334: 盘式记法
- Page335: KL散度(414)
- Page337: 话题模型
- Page337: 平均场
- Page337: 隐狄利克雷分配模型
- Page340: 非参数化方法

## 第15章 规则学习
- Page347: 规则
- Page347: 规则学习
- Page347: 逻辑文字
- Page348: 冲突消解
- Page348: 待续规则
- Page348: 命题规则
- Page348: 默认规则
- Page348: 缺省规则
- Page348: 一阶规则
- Page348: 优先级规则
- Page348: 元规则
- Page348: 原子命题
- Page349: 序贯覆盖
- Page350: 特化
- Page352: 似然率
- Page353: RIPPER
- Page357: ILP(364)
- Page357: 归纳逻辑程序设计(364)
- Page358: 最小一般泛化
- Page359: 归纳
- Page359: 逆归结
- Page359: 演绎
- Page361: 合一
- Page361: 置换
- Page361: 最一般合一置换
- Page362: 归结商
- Page363: 关系学习
- Page364: 统计关系学习


## 第16章 强化学习
- Page371: MDP
- Page371: 奖赏
- Page371: 马尔科夫决策过程
- Page371: 强化学习
- Page371: 再励学习
- Page372: 策略
- Page373: K-摇臂赌博机
- Page374: ϵ-贪心
- Page374: 探索-利用窘境
- Page375: Softmax
- Page377: 有模型学习
- Page377: 状态-动作值函数
- Page377: 状态值函数
- Page380: Bellman等式
- Page381: 策略迭代
- Page382: 免模型学习
- Page382: 值迭代
- Page386: TD学习(393)
- Page386: 时序差分学习(393)
- Page387: Q-学习(393)
- Page387: Sarsa算法(390)
- Page388: 表格值函数
- Page388: 值函数近似
- Page390: 模仿学习
- Page391: 逆强化学习
- Page393: 近似动态规划

## 附录
- Page399: 行列式
- Page399: 迹
- Page400: Frobenius范数
- Page402: 低秩矩阵近似
- Page403: 拉格朗日乘子法
- Page405: 对偶函数
- Page406: 二次规划
- Page407: 半正定规划
- Page409: 伯努利分布
- Page409: 均匀分布
- Page410: 多项分布
- Page410: 二项分布
- Page411: 贝塔分布
- Page412: 狄利克雷分布
- Page412: 高斯分布
- Page412: 正态分布
- Page413: 共轭分布
- Page414: 相对熵
- Page414: 信息散度
- Page415: 交叉熵
- Page415: 熵
